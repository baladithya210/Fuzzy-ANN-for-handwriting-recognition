# -*- coding: utf-8 -*-
"""Handwriting recognition

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19kXgK-v7Palg0KCM11G37-hX_63NFSSs

**Fuzzy-ANN System for Handwriting Recognition**

Objective:
Implement a Fuzzy Neural Network to improve the accuracy and adaptability of handwriting
recognition systems
"""

#dataset loading and normilization
import numpy as np
from tensorflow.keras.datasets import mnist

# Load and normalize the MNIST dataset
(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0    #Dividing by 255.0 scales the pixel values to the range [0, 1]

# One-hot encode the labels
# One-hot encoding converts these integer labels into binary arrays of length 10.
y_train = np.eye(10)[y_train]
y_test = np.eye(10)[y_test]

x_train.shape, x_test.shape ,y_test.shape, y_train.shape

import matplotlib.pyplot as plt

def show_images_grid(images, labels, rows=2, cols=10):
    # Convert one-hot encoded labels to numeric labels
    numeric_labels = np.argmax(labels, axis=1)

    fig, axes = plt.subplots(rows, cols, figsize=(12, 4))
    axes = axes.flatten()  # Flatten the 2D array of axes for easier iteration

    for i in range(rows * cols):
        # Display the image in grayscale
        axes[i].imshow(images[i], cmap='gray')
        axes[i].set_title(f"Label: {numeric_labels[i]}")  # Display the numeric label
        axes[i].axis('off')  # Remove axis ticks

    plt.tight_layout()  # Adjust spacing to prevent overlap
    plt.show()

# Display a grid of 20 images (first 20 training examples)
show_images_grid(x_train, y_train, rows=2, cols=10)

# Fuzzification of pixels based on their degree of truth (light, medium, dark)
def fuzzify_pixel(value):
    # Define thresholds for fuzzification
    if value <= 0.33:  # If the pixel intensity is low, categorize it as "light"
        return "light"
    elif value <= 0.66:  # If the pixel intensity is moderate, categorize it as "medium"
        return "medium"
    else:  # If the pixel intensity is high, categorize it as "dark"
        return "dark"

# Create a mapping to convert fuzzified categories into numerical values
encoding_map = {
    "light": 0,    # Assign the category "light" a numerical value of 0
    "medium": 1,   # Assign the category "medium" a numerical value of 1
    "dark": 2      # Assign the category "dark" a numerical value of 2
}

# Define a function to fuzzify and encode an entire image
def fuzzify_and_encode_image(image):
    # Use np.vectorize to apply fuzzification and encoding to each pixel in the image
    fuzzified_image = np.vectorize(lambda v: encoding_map[fuzzify_pixel(v)])(image)
    return fuzzified_image

# Process the training and testing datasets
# Apply the fuzzify_and_encode_image function to each image in x_train and x_test
encoded_x_train = np.array([fuzzify_and_encode_image(img) for img in x_train])
encoded_x_test = np.array([fuzzify_and_encode_image(img) for img in x_test])

# Flatten encoded images
# Reshape the 2D images (28x28) into 1D arrays for compatibility with machine learning models
encoded_x_train = encoded_x_train.reshape(encoded_x_train.shape[0], -1) # Fuzzify and encode each image in the training dataset
encoded_x_test = encoded_x_test.reshape(encoded_x_test.shape[0], -1)   # Fuzzify and encode each image in the testing dataset

# ANN parameters and processing the encoded image through the ann
input_size = 784   # Number of input neurons (28x28 flattened image)
hidden_size1 = 256 # Number of neurons in the first hidden layer
hidden_size2 = 128 # Number of neurons in the second hidden layer
output_size = 10   # Number of output neurons (10 classes for digits 0-9)
np.random.seed(42)  # Sets the random seed to ensure reproducibility of random numbers
# Initialize weights and biases for the first hidden layer
weights_input_hidden1 = np.random.randn(input_size, hidden_size1) * np.sqrt(2.0 / input_size)  # Xavier initialization
bias_hidden1 = np.zeros((1, hidden_size1))
# Initialize weights and biases for the second hidden layer
weights_hidden1_hidden2 = np.random.randn(hidden_size1, hidden_size2) * np.sqrt(2.0 / hidden_size1)
bias_hidden2 = np.zeros((1, hidden_size2))
# Initialize weights and biases for the output layer
weights_hidden_output = np.random.randn(hidden_size2, output_size) * np.sqrt(2.0 / hidden_size2)
bias_output = np.zeros((1, output_size))

# Activation functions
def relu(x):
    return np.maximum(0, x)  # Returns the maximum of 0 and x, ensuring non-negative outputs

def relu_derivative(x):
    return (x > 0).astype(float)   # Returns 1 for positive values of x, and 0 for negative


def softmax(x):
    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))  # Stability fix to prevent overflow
    return exp_x / np.sum(exp_x, axis=1, keepdims=True)   # Normalizes to output probabilities

# Loss function
def compute_loss(y_true, y_pred):  # Computes the loss using the cross-entropy formula
    m = y_true.shape[0]
    return -np.sum(y_true * np.log(y_pred + 1e-9)) / m  # Avoid log(0)

# Forward propagation
def forward_propagation(x):
    z1 = np.dot(x, weights_input_hidden1) + bias_hidden1    # First hidden layer: Compute the weighted sum (z1) and apply ReLU activation (a1)
    a1 = relu(z1)    # Apply ReLU activation to z1
    z2 = np.dot(a1, weights_hidden1_hidden2) + bias_hidden2   # Second hidden layer: Compute the weighted sum (z2) and apply ReLU activation (a2)
    a2 = relu(z2)     # Apply ReLU activation to z2
    z3 = np.dot(a2, weights_hidden_output) + bias_output   # Output layer: Compute the weighted sum (z3) and apply Softmax activation (a3)
    a3 = softmax(z3)     # Apply Softmax activation to z3 for classification
    return z1, a1, z2, a2, z3, a3

# Backpropagation
def back_propagation(x, y, z1, a1, z2, a2, z3, a3):
    m = y.shape[0]  # Number of training examples in the batch
    dz3 = a3 - y  # Output layer error: difference between predictions (a3) and true labels (y)
    dweights_hidden_output = np.dot(a2.T, dz3) / m  # Gradient for weights between the second hidden layer and the output layer
    dbias_output = np.sum(dz3, axis=0, keepdims=True) / m  # Gradient for biases in the output layer
    # Backpropagate to the second hidden layer
    da2 = np.dot(dz3, weights_hidden_output.T)  # Propagate the error back to the second hidden layer
    dz2 = da2 * relu_derivative(z2)  # Element-wise multiply by ReLU derivative of z2
    dweights_hidden1_hidden2 = np.dot(a1.T, dz2) / m  # Gradient for weights between the first and second hidden layers
    dbias_hidden2 = np.sum(dz2, axis=0, keepdims=True) / m  # Gradient for biases in the second hidden layer
    # Backpropagate to the first hidden layer
    da1 = np.dot(dz2, weights_hidden1_hidden2.T)  # Propagate the error back to the first hidden layer
    dz1 = da1 * relu_derivative(z1)  # Element-wise multiply by ReLU derivative of z1
    dweights_input_hidden1 = np.dot(x.T, dz1) / m  # Gradient for weights between the input layer and the first hidden layer
    dbias_hidden1 = np.sum(dz1, axis=0, keepdims=True) / m  # Gradient for biases in the first hidden layer
    # Return the gradients for all layers' weights and biases
    return dweights_input_hidden1, dbias_hidden1, dweights_hidden1_hidden2, dbias_hidden2, dweights_hidden_output, dbias_output


# Update parameters
def update_parameters(dweights_input_hidden1, dbias_hidden1, dweights_hidden1_hidden2, dbias_hidden2, dweights_hidden_output, dbias_output, learning_rate):
    global weights_input_hidden1, bias_hidden1, weights_hidden1_hidden2, bias_hidden2, weights_hidden_output, bias_output
    # Update weights and biases for the first hidden layer
    weights_input_hidden1 -= learning_rate * dweights_input_hidden1
    # The new value of weights_input_hidden1 is calculated by subtracting the product of the learning rate and the gradient.
    # This reduces the loss function in the direction of the negative gradient.
    bias_hidden1 -= learning_rate * dbias_hidden1
    # Update weights and biases for the second hidden layer
    weights_hidden1_hidden2 -= learning_rate * dweights_hidden1_hidden2
    # weights_hidden1_hidden2 is updated by adjusting based on the gradient of the loss with respect to these weights.
    bias_hidden2 -= learning_rate * dbias_hidden2
    # Update weights and biases for the output layer
    weights_hidden_output -= learning_rate * dweights_hidden_output
    # weights_hidden_output is updated in the same manner, using the gradients specific to the output layer.
    bias_output -= learning_rate * dbias_output

# Training the model for 20 epochs
epochs = 20  # Number of epochs to train the model
learning_rate = 0.001  # The learning rate for parameter updates
batch_size = 128  # Mini-batch size for training
# Loop through each epoch
for epoch in range(epochs):
    # Loop through the training data in mini-batches
    for i in range(0, encoded_x_train.shape[0], batch_size):
        # Create mini-batches
        x_batch = encoded_x_train[i:i + batch_size]  # Input batch
        y_batch = y_train[i:i + batch_size]  # Output batch

        # Perform forward propagation to get activations and predictions
        z1, a1, z2, a2, z3, a3 = forward_propagation(x_batch)
        loss = compute_loss(y_batch, a3)  # Compute the loss based on predicted and true values
        grads = back_propagation(x_batch, y_batch, z1, a1, z2, a2, z3, a3)   # Calculate gradients using backpropagation
        update_parameters(*grads, learning_rate)  # Update parameters using the calculated gradients and learning rate

    # Compute epoch accuracy
    _, _, _, _, _, a3_train = forward_propagation(encoded_x_train)   # Get the final output activations for the training set
    train_predictions = np.argmax(a3_train, axis=1)    # Get the predicted class labels by taking the argmax of the output probabilities
    train_true_labels = np.argmax(y_train, axis=1)     # Get the true class labels from one-hot encoded labels
    train_accuracy = np.mean(train_predictions == train_true_labels)    # Compute accuracy by comparing predicted and true labels

    print(f"Epoch {epoch + 1}/{epochs}, Loss: {loss:.4f}, Accuracy: {train_accuracy:.4f}")   # Print loss and accuracy for the current epoch

# Evaluate the model on test data
_, _, _, _, _, a3_test = forward_propagation(encoded_x_test)  # Get the output activations for the test set
test_loss = compute_loss(y_test, a3_test)  # Compute the loss for the test set
test_predictions = np.argmax(a3_test, axis=1)  # Get the predicted labels from the output activations
test_true_labels = np.argmax(y_test, axis=1)  # Extract the true labels from the one-hot encoded test labels
test_accuracy = np.mean(test_predictions == test_true_labels)  # Calculate accuracy by comparing predicted and true labels

# Print the test loss and accuracy
print(f"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}")

# Perform forward propagation on test data to get predictions
_, _, _, _, _, a3_test = forward_propagation(encoded_x_test)
test_predictions = np.argmax(a3_test, axis=1)  # Predicted labels
test_true_labels = np.argmax(y_test, axis=1)   # True labels

# Select a few samples to display
num_samples = 10  # Number of test samples to visualize
indices = np.random.choice(x_test.shape[0], num_samples, replace=False)  # Randomly select test sample indices
selected_images = x_test[indices]  # Get the images
selected_true_labels = test_true_labels[indices]  # True labels of the selected images
selected_predicted_labels = test_predictions[indices]  # Predicted labels of the selected images

# Plot the selected samples
plt.figure(figsize=(15, 5))  # Create a larger figure for more samples
for i, (image, true_label, predicted_label) in enumerate(zip(selected_images, selected_true_labels, selected_predicted_labels)):
    plt.subplot(1, num_samples, i + 1)  # Create a subplot for each image
    plt.imshow(image, cmap='gray')  # Display the image in grayscale
    plt.title(f"True: {true_label}\nPred: {predicted_label}")  # Show true and predicted labels
    plt.axis('off')  # Turn off the axis for a cleaner look
plt.tight_layout()  # Adjust layout to avoid overlapping
plt.show()